{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = np.loadtxt('../assets/training_dataset.txt', dtype=float)\n",
    "test_ds = np.loadtxt('../assets/test_dataset.txt', dtype=float)\n",
    "m, n = train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract(train_data):\n",
    "    data_list = list()\n",
    "    label_list = list()\n",
    "    for val in train_data:\n",
    "        data_list.append(val[0:784])\n",
    "        label_list.append(int(val[784]))\n",
    "    data_list = np.array(data_list)\n",
    "    label_list = np.array(label_list)\n",
    "    return data_list, label_list\n",
    "\n",
    "np.random.shuffle(train_ds)\n",
    "\n",
    "train_dataset, train_label = data_extract(train_ds)\n",
    "train_dataset = train_dataset.T\n",
    "x_train = train_dataset[0:784]\n",
    "# x_train = x_train / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 5, 9, ..., 2, 8, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables():\n",
    "    w1 = np.random.rand(200, 784) - 0.5 # random number between 0 and 0.5 (weight from input to hidden)\n",
    "    b1 = np.random.rand(200, 1) - 0.5 # bias of first layer\n",
    "    w2 = np.random.rand(10, 200) - 0.5 # weight for second hidden layer\n",
    "    b2 = np.random.rand(10, 1) - 0.5 # bias for second layer\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "\n",
    "def relu(input_value):\n",
    "    return np.maximum(input_value, 0)\n",
    "\n",
    "def sigmoid(input_value):\n",
    "    return 1 / (1 + np.exp(-input_value))\n",
    "\n",
    "def sigmoid_prime(s):\n",
    "    #derivative of sigmoid\n",
    "    return sigmoid(s) * (1 - sigmoid(s))\n",
    "\n",
    "def softmax_activation(z):\n",
    "    # activate = np.exp(z) / sum(np.exp(z))\n",
    "    # return activate\n",
    "    A = np.exp(z) / sum(np.exp(z))\n",
    "    return A \n",
    "\n",
    "\n",
    "def feed_forward(w1, b1, w2, b2, input_value):\n",
    "    img1 = w1.dot(input_value) + 1\n",
    "    activate1 = relu(img1)\n",
    "    img2 = w2.dot(activate1) + 1\n",
    "    output = softmax_activation(img2)\n",
    "    return img1, activate1, img2, output\n",
    "\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return z > 0\n",
    "\n",
    "\n",
    "def one_hot_fn(label):\n",
    "    one_hot_label = np.zeros((label.size, label.max() + 1))\n",
    "    one_hot_label[np.arange(label.size), label] = 1\n",
    "    one_hot_label = one_hot_label.T\n",
    "    return one_hot_label\n",
    "    \n",
    "\n",
    "def back_propagation(img1, a1, img2, a2, w1, w2, input_value, label):\n",
    "    one_hot_label = one_hot_fn(label)\n",
    "    d_img2 = a2 - one_hot_label # error\n",
    "    d_w2 = 1 / m * d_img2.dot(a1.T)\n",
    "    d_b2 = 1 / m * np.sum(d_img2)\n",
    "    d_img1 = w2.T.dot(d_img2) * relu_derivative(img1)\n",
    "    d_w1 = 1 / m * d_img1.dot(input_value.T)\n",
    "    d_b1 = 1 / m * np.sum(d_img1)\n",
    "    return d_w1, d_b1, d_w2, d_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89202898 0.80723053 0.49199795 0.97416138 0.63228619 0.85026618\n",
      "  0.51141408 0.15098571 0.42288366 0.68712664]\n",
      " [0.96034351 0.09396997 0.98455475 0.01675594 0.42848981 0.23171726\n",
      "  0.62539021 0.96820137 0.02310213 0.82858635]]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.rand(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_fn(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict(output):\n",
    "    return np.argmax(output, 0)\n",
    "\n",
    "def get_accuracy(predict, label):\n",
    "    print(predict, label)\n",
    "    return np.sum(predict == label) / label.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w1, b1, w2, b2, d_w1, d_b1, d_w2, d_b2, alpha):\n",
    "    w1 = w1 - alpha * d_w1\n",
    "    b1 = b1 - alpha * d_b1\n",
    "    w2 = w2 - alpha * d_w2\n",
    "    b2 = b2 - alpha * d_b2\n",
    "    return w1, b1, w2, b2    \n",
    "\n",
    "\n",
    "def gradient_descent(img, label, alpha, epochs):\n",
    "    w1, b1, w2, b2 = init_variables()\n",
    "    for i in range(epochs):\n",
    "        img1, act1, img2, act2 = feed_forward(w1, b1, w2, b2, img)\n",
    "        d_w1, d_b1, d_w2, d_b2 = back_propagation(img1, act1, img2, act2, w1, w2, img, label)\n",
    "        w1, b1, w2, b2 = update_parameters(w1, b1, w2, b2, d_w1, d_b1, d_w2, d_b2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(f'Iteration: {i}')\n",
    "            prediction = get_predict(act2)\n",
    "            print(f'Rate: {get_accuracy(prediction, label)}')\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "[5 5 8 ... 7 5 7] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.084\n",
      "Iteration: 10\n",
      "[5 3 8 ... 2 5 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.10875\n",
      "Iteration: 20\n",
      "[5 4 8 ... 2 5 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.18875\n",
      "Iteration: 30\n",
      "[9 5 4 ... 2 5 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.26925\n",
      "Iteration: 40\n",
      "[9 5 4 ... 2 5 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.34625\n",
      "Iteration: 50\n",
      "[9 5 4 ... 2 5 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.4025\n",
      "Iteration: 60\n",
      "[9 5 4 ... 2 5 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.4565\n",
      "Iteration: 70\n",
      "[9 5 4 ... 2 9 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.4985\n",
      "Iteration: 80\n",
      "[9 5 4 ... 2 9 8] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.52875\n",
      "Iteration: 90\n",
      "[9 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.558\n",
      "Iteration: 100\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.5795\n",
      "Iteration: 110\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.60375\n",
      "Iteration: 120\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.6205\n",
      "Iteration: 130\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.637\n",
      "Iteration: 140\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.64975\n",
      "Iteration: 150\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.66375\n",
      "Iteration: 160\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.6745\n",
      "Iteration: 170\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.68325\n",
      "Iteration: 180\n",
      "[8 5 4 ... 2 9 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.691\n",
      "Iteration: 190\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.7005\n",
      "Iteration: 200\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.706\n",
      "Iteration: 210\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.7115\n",
      "Iteration: 220\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.72\n",
      "Iteration: 230\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.7265\n",
      "Iteration: 240\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.731\n",
      "Iteration: 250\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.735\n",
      "Iteration: 260\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.741\n",
      "Iteration: 270\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.74575\n",
      "Iteration: 280\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.749\n",
      "Iteration: 290\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.756\n",
      "Iteration: 300\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.7595\n",
      "Iteration: 310\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.76325\n",
      "Iteration: 320\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.7645\n",
      "Iteration: 330\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.76875\n",
      "Iteration: 340\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.77125\n",
      "Iteration: 350\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.77425\n",
      "Iteration: 360\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.77725\n",
      "Iteration: 370\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.78\n",
      "Iteration: 380\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.7845\n",
      "Iteration: 390\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.788\n",
      "Iteration: 400\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.791\n",
      "Iteration: 410\n",
      "[8 5 4 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.79275\n",
      "Iteration: 420\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.796\n",
      "Iteration: 430\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8\n",
      "Iteration: 440\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.80375\n",
      "Iteration: 450\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.80575\n",
      "Iteration: 460\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.80775\n",
      "Iteration: 470\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.80975\n",
      "Iteration: 480\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8125\n",
      "Iteration: 490\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.81425\n",
      "Iteration: 500\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8175\n",
      "Iteration: 510\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.819\n",
      "Iteration: 520\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.82075\n",
      "Iteration: 530\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.82175\n",
      "Iteration: 540\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8235\n",
      "Iteration: 550\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.825\n",
      "Iteration: 560\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8265\n",
      "Iteration: 570\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.828\n",
      "Iteration: 580\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8295\n",
      "Iteration: 590\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.83025\n",
      "Iteration: 600\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.83125\n",
      "Iteration: 610\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8325\n",
      "Iteration: 620\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.83425\n",
      "Iteration: 630\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.836\n",
      "Iteration: 640\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.837\n",
      "Iteration: 650\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8385\n",
      "Iteration: 660\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8395\n",
      "Iteration: 670\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8405\n",
      "Iteration: 680\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.84175\n",
      "Iteration: 690\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.84325\n",
      "Iteration: 700\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.84525\n",
      "Iteration: 710\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.84625\n",
      "Iteration: 720\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.84775\n",
      "Iteration: 730\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8485\n",
      "Iteration: 740\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.849\n",
      "Iteration: 750\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.84925\n",
      "Iteration: 760\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.84975\n",
      "Iteration: 770\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8505\n",
      "Iteration: 780\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.8515\n",
      "Iteration: 790\n",
      "[8 5 2 ... 2 8 1] [8 5 9 ... 2 8 1]\n",
      "Rate: 0.853\n"
     ]
    }
   ],
   "source": [
    "#TODO: separate train dataset and create new ds\n",
    "\n",
    "train = gradient_descent(x_train, train_label, 0.01, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(y): \n",
    "    max_out = np.amax(y)\n",
    "    for i, j in enumerate(y): \n",
    "        if j == max_out:\n",
    "            return i\n",
    "        \n",
    "def identify(y):\n",
    "    results = []\n",
    "    index = 0\n",
    "    for data in y:\n",
    "        max_output = np.amax(data)\n",
    "        # for i, j in enumerate(data): \n",
    "        #     if j == max_output:\n",
    "        #         results.append(i)\n",
    "        count = 0\n",
    "        for x in (data): \n",
    "            if x == max_output:\n",
    "                results.append(count)\n",
    "            count += 1\n",
    "    return np.asarray(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e79c9b4dc955595e7f11f70f4f1c2000e30fde47a19708ab60dcdade70e6665e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
