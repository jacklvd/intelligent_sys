{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "       \n",
    "    # compute the output Y of a layer for a give input X\n",
    "    def forward_prop(self, input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # computes dE/dX for a given dE/dY (and update parameter if any)\n",
    "    def backward_prop(self, output_err, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(input_value):\n",
    "    return np.maximum(input_value, 0)\n",
    "\n",
    "\n",
    "def softmax_activation(x):\n",
    "    # activate = np.exp(z) / sum(np.exp(z))\n",
    "    # return activate\n",
    "    output = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return output\n",
    "\n",
    "def relu_prime(x):\n",
    "    return x > 0\n",
    "\n",
    "def one_hot_fn(y_label):\n",
    "    one_hot_label = np.zeros((y_label.size, y_label.max() + 1))\n",
    "    one_hot_label[np.arange(y_label.size), y_label] = 1\n",
    "    one_hot_label = one_hot_label.T\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        # self.y = y\n",
    "        # self.epochs = epochs\n",
    "        # self.learning_rate = learning_rate\n",
    "        # self.w = np.random.randn(input_size, output_size) * np.sqrt(1 / (input_size + output_size))\n",
    "        # self.b = 1\n",
    "        self.w = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.b = np.random.rand(1, output_size) - 0.5\n",
    "        \n",
    "    # return output for a given input\n",
    "    def forward_prop(self, input_ds):\n",
    "        self.input = input_ds\n",
    "        # self.output = np.dot(self.input, self.w) + self.b\n",
    "        self.output = np.dot(self.input, self.w) + self.b\n",
    "        return self.output\n",
    "    \n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_err=dE/dX\n",
    "    def backward_prop(self, o_err, learning_rate):\n",
    "        # print(o_err.shape)\n",
    "        # print(self.input.shape)\n",
    "        input_err = np.dot(o_err, self.w.T)\n",
    "        # print(input_err.shape)\n",
    "        if len(self.input.shape) == 1:\n",
    "            self.input = np.expand_dims(self.input, axis=0)\n",
    "        w_err = np.dot(self.input.T, o_err)\n",
    "        # w_err = self.input.T.dot(o_err)\n",
    "        # dB = o_err\n",
    "        \n",
    "        # self.output_error = self.y - self.output\n",
    "        \n",
    "        # update parameters\n",
    "        self.w -= learning_rate * w_err + 0.5 * self.w\n",
    "        self.b -= learning_rate * o_err\n",
    "        # print(self.b)\n",
    "        return input_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "        \n",
    "    # return the activation input\n",
    "    def forward_prop(self, input_ds):\n",
    "        self.input = input_ds\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "    \n",
    "    # return input_error=dE/dX for a given output_error=dE/dY\n",
    "    def backward_prop(self, output_err, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its prime\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def deri_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_prop(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def gradient_descent(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_prop(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_prop(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            # if i % 10 == 0:\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = np.loadtxt('../assets/training_dataset.txt', dtype=float)\n",
    "test_ds = np.loadtxt('../assets/test_dataset.txt', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract(train_data):\n",
    "    data_list = list()\n",
    "    label_list = list()\n",
    "    for val in train_data:\n",
    "        data_list.append(val[0:784])\n",
    "        label_list.append(int(val[784]))\n",
    "    data_list = np.array(data_list)\n",
    "    label_list = np.array(label_list)\n",
    "    return data_list, label_list\n",
    "\n",
    "np.random.shuffle(train_ds)\n",
    "\n",
    "train_dataset, y_label = data_extract(train_ds)\n",
    "train_dataset = train_dataset\n",
    "x_train = train_dataset\n",
    "# x_train = x_train.reshape(x_train[0], 1, 28*28)\n",
    "x_train = x_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "net = Network()\n",
    "net.add(ConnectLayer(784, 150))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(Activation(relu, relu_prime))\n",
    "net.add(ConnectLayer(150, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(Activation(relu, relu_prime))\n",
    "net.add(ConnectLayer(50, 10))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(Activation(sigmoid, deri_sigmoid))\n",
    "m, n = x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 784)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/500   error=20.658304\n",
      "epoch 2/500   error=20.517911\n",
      "epoch 3/500   error=20.510354\n",
      "epoch 4/500   error=20.507308\n",
      "epoch 5/500   error=20.505652\n",
      "epoch 6/500   error=20.504608\n",
      "epoch 7/500   error=20.503891\n",
      "epoch 8/500   error=20.503367\n",
      "epoch 9/500   error=20.502967\n",
      "epoch 10/500   error=20.502652\n",
      "epoch 11/500   error=20.502398\n",
      "epoch 12/500   error=20.502188\n",
      "epoch 13/500   error=20.502012\n",
      "epoch 14/500   error=20.501862\n",
      "epoch 15/500   error=20.501733\n",
      "epoch 16/500   error=20.501621\n",
      "epoch 17/500   error=20.501522\n",
      "epoch 18/500   error=20.501435\n",
      "epoch 19/500   error=20.501357\n",
      "epoch 20/500   error=20.501287\n",
      "epoch 21/500   error=20.501224\n",
      "epoch 22/500   error=20.501167\n",
      "epoch 23/500   error=20.501115\n",
      "epoch 24/500   error=20.501067\n",
      "epoch 25/500   error=20.501024\n",
      "epoch 26/500   error=20.500983\n",
      "epoch 27/500   error=20.500946\n",
      "epoch 28/500   error=20.500912\n",
      "epoch 29/500   error=20.500880\n",
      "epoch 30/500   error=20.500850\n",
      "epoch 31/500   error=20.500822\n",
      "epoch 32/500   error=20.500796\n",
      "epoch 33/500   error=20.500771\n",
      "epoch 34/500   error=20.500748\n",
      "epoch 35/500   error=20.500726\n",
      "epoch 36/500   error=20.500706\n",
      "epoch 37/500   error=20.500686\n",
      "epoch 38/500   error=20.500668\n",
      "epoch 39/500   error=20.500651\n",
      "epoch 40/500   error=20.500634\n",
      "epoch 41/500   error=20.500619\n",
      "epoch 42/500   error=20.500604\n",
      "epoch 43/500   error=20.500589\n",
      "epoch 44/500   error=20.500576\n",
      "epoch 45/500   error=20.500563\n",
      "epoch 46/500   error=20.500550\n",
      "epoch 47/500   error=20.500539\n",
      "epoch 48/500   error=20.500527\n",
      "epoch 49/500   error=20.500516\n",
      "epoch 50/500   error=20.500506\n",
      "epoch 51/500   error=20.500496\n",
      "epoch 52/500   error=20.500486\n",
      "epoch 53/500   error=20.500477\n",
      "epoch 54/500   error=20.500468\n",
      "epoch 55/500   error=20.500459\n",
      "epoch 56/500   error=20.500451\n",
      "epoch 57/500   error=20.500443\n",
      "epoch 58/500   error=20.500435\n",
      "epoch 59/500   error=20.500428\n",
      "epoch 60/500   error=20.500421\n",
      "epoch 61/500   error=20.500414\n",
      "epoch 62/500   error=20.500407\n",
      "epoch 63/500   error=20.500401\n",
      "epoch 64/500   error=20.500394\n",
      "epoch 65/500   error=20.500388\n",
      "epoch 66/500   error=20.500382\n",
      "epoch 67/500   error=20.500376\n",
      "epoch 68/500   error=20.500371\n",
      "epoch 69/500   error=20.500365\n",
      "epoch 70/500   error=20.500360\n",
      "epoch 71/500   error=20.500355\n",
      "epoch 72/500   error=20.500350\n",
      "epoch 73/500   error=20.500345\n",
      "epoch 74/500   error=20.500341\n",
      "epoch 75/500   error=20.500336\n",
      "epoch 76/500   error=20.500332\n",
      "epoch 77/500   error=20.500327\n",
      "epoch 78/500   error=20.500323\n",
      "epoch 79/500   error=20.500319\n",
      "epoch 80/500   error=20.500315\n",
      "epoch 81/500   error=20.500311\n",
      "epoch 82/500   error=20.500307\n",
      "epoch 83/500   error=20.500303\n",
      "epoch 84/500   error=20.500300\n",
      "epoch 85/500   error=20.500296\n",
      "epoch 86/500   error=20.500293\n",
      "epoch 87/500   error=20.500289\n",
      "epoch 88/500   error=20.500286\n",
      "epoch 89/500   error=20.500283\n",
      "epoch 90/500   error=20.500280\n",
      "epoch 91/500   error=20.500277\n",
      "epoch 92/500   error=20.500273\n",
      "epoch 93/500   error=20.500271\n",
      "epoch 94/500   error=20.500268\n",
      "epoch 95/500   error=20.500265\n",
      "epoch 96/500   error=20.500262\n",
      "epoch 97/500   error=20.500259\n",
      "epoch 98/500   error=20.500257\n",
      "epoch 99/500   error=20.500254\n",
      "epoch 100/500   error=20.500251\n",
      "epoch 101/500   error=20.500249\n",
      "epoch 102/500   error=20.500247\n",
      "epoch 103/500   error=20.500244\n",
      "epoch 104/500   error=20.500242\n",
      "epoch 105/500   error=20.500239\n",
      "epoch 106/500   error=20.500237\n",
      "epoch 107/500   error=20.500235\n",
      "epoch 108/500   error=20.500233\n",
      "epoch 109/500   error=20.500231\n",
      "epoch 110/500   error=20.500229\n",
      "epoch 111/500   error=20.500226\n",
      "epoch 112/500   error=20.500224\n",
      "epoch 113/500   error=20.500222\n",
      "epoch 114/500   error=20.500220\n",
      "epoch 115/500   error=20.500219\n",
      "epoch 116/500   error=20.500217\n",
      "epoch 117/500   error=20.500215\n",
      "epoch 118/500   error=20.500213\n",
      "epoch 119/500   error=20.500211\n",
      "epoch 120/500   error=20.500209\n",
      "epoch 121/500   error=20.500208\n",
      "epoch 122/500   error=20.500206\n",
      "epoch 123/500   error=20.500204\n",
      "epoch 124/500   error=20.500203\n",
      "epoch 125/500   error=20.500201\n",
      "epoch 126/500   error=20.500199\n",
      "epoch 127/500   error=20.500198\n",
      "epoch 128/500   error=20.500196\n",
      "epoch 129/500   error=20.500195\n",
      "epoch 130/500   error=20.500193\n",
      "epoch 131/500   error=20.500192\n",
      "epoch 132/500   error=20.500190\n",
      "epoch 133/500   error=20.500189\n",
      "epoch 134/500   error=20.500187\n",
      "epoch 135/500   error=20.500186\n",
      "epoch 136/500   error=20.500185\n",
      "epoch 137/500   error=20.500183\n",
      "epoch 138/500   error=20.500182\n",
      "epoch 139/500   error=20.500181\n",
      "epoch 140/500   error=20.500179\n",
      "epoch 141/500   error=20.500178\n",
      "epoch 142/500   error=20.500177\n",
      "epoch 143/500   error=20.500176\n",
      "epoch 144/500   error=20.500174\n",
      "epoch 145/500   error=20.500173\n",
      "epoch 146/500   error=20.500172\n",
      "epoch 147/500   error=20.500171\n",
      "epoch 148/500   error=20.500170\n",
      "epoch 149/500   error=20.500168\n",
      "epoch 150/500   error=20.500167\n",
      "epoch 151/500   error=20.500166\n",
      "epoch 152/500   error=20.500165\n",
      "epoch 153/500   error=20.500164\n",
      "epoch 154/500   error=20.500163\n",
      "epoch 155/500   error=20.500162\n",
      "epoch 156/500   error=20.500161\n",
      "epoch 157/500   error=20.500160\n",
      "epoch 158/500   error=20.500159\n",
      "epoch 159/500   error=20.500158\n",
      "epoch 160/500   error=20.500157\n",
      "epoch 161/500   error=20.500156\n",
      "epoch 162/500   error=20.500155\n",
      "epoch 163/500   error=20.500154\n",
      "epoch 164/500   error=20.500153\n",
      "epoch 165/500   error=20.500152\n",
      "epoch 166/500   error=20.500151\n",
      "epoch 167/500   error=20.500150\n",
      "epoch 168/500   error=20.500149\n",
      "epoch 169/500   error=20.500148\n",
      "epoch 170/500   error=20.500148\n",
      "epoch 171/500   error=20.500147\n",
      "epoch 172/500   error=20.500146\n",
      "epoch 173/500   error=20.500145\n",
      "epoch 174/500   error=20.500144\n",
      "epoch 175/500   error=20.500143\n",
      "epoch 176/500   error=20.500143\n",
      "epoch 177/500   error=20.500142\n",
      "epoch 178/500   error=20.500141\n",
      "epoch 179/500   error=20.500140\n",
      "epoch 180/500   error=20.500139\n",
      "epoch 181/500   error=20.500139\n",
      "epoch 182/500   error=20.500138\n",
      "epoch 183/500   error=20.500137\n",
      "epoch 184/500   error=20.500136\n",
      "epoch 185/500   error=20.500136\n",
      "epoch 186/500   error=20.500135\n",
      "epoch 187/500   error=20.500134\n",
      "epoch 188/500   error=20.500133\n",
      "epoch 189/500   error=20.500133\n",
      "epoch 190/500   error=20.500132\n",
      "epoch 191/500   error=20.500131\n",
      "epoch 192/500   error=20.500131\n",
      "epoch 193/500   error=20.500130\n",
      "epoch 194/500   error=20.500129\n",
      "epoch 195/500   error=20.500129\n",
      "epoch 196/500   error=20.500128\n",
      "epoch 197/500   error=20.500127\n",
      "epoch 198/500   error=20.500127\n",
      "epoch 199/500   error=20.500126\n",
      "epoch 200/500   error=20.500125\n",
      "epoch 201/500   error=20.500125\n",
      "epoch 202/500   error=20.500124\n",
      "epoch 203/500   error=20.500124\n",
      "epoch 204/500   error=20.500123\n",
      "epoch 205/500   error=20.500122\n",
      "epoch 206/500   error=20.500122\n",
      "epoch 207/500   error=20.500121\n",
      "epoch 208/500   error=20.500121\n",
      "epoch 209/500   error=20.500120\n",
      "epoch 210/500   error=20.500119\n",
      "epoch 211/500   error=20.500119\n",
      "epoch 212/500   error=20.500118\n",
      "epoch 213/500   error=20.500118\n",
      "epoch 214/500   error=20.500117\n",
      "epoch 215/500   error=20.500117\n",
      "epoch 216/500   error=20.500116\n",
      "epoch 217/500   error=20.500116\n",
      "epoch 218/500   error=20.500115\n",
      "epoch 219/500   error=20.500114\n",
      "epoch 220/500   error=20.500114\n",
      "epoch 221/500   error=20.500113\n",
      "epoch 222/500   error=20.500113\n",
      "epoch 223/500   error=20.500112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [156], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m net\u001b[39m.\u001b[39muse(mse, mse_prime)\n\u001b[1;32m----> 2\u001b[0m net\u001b[39m.\u001b[39;49mgradient_descent(x_train, y_label, epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# test on 3 samples\u001b[39;00m\n\u001b[0;32m      5\u001b[0m out \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mpredict(y_label[\u001b[39m0\u001b[39m:\u001b[39m3\u001b[39m])\n",
      "Cell \u001b[1;32mIn [151], line 52\u001b[0m, in \u001b[0;36mNetwork.gradient_descent\u001b[1;34m(self, x_train, y_train, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     50\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_prime(y_train[j], output)\n\u001b[0;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m---> 52\u001b[0m         error \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mbackward_prop(error, learning_rate)\n\u001b[0;32m     54\u001b[0m \u001b[39m# calculate average error on all samples\u001b[39;00m\n\u001b[0;32m     55\u001b[0m err \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m samples\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.use(mse, mse_prime)\n",
    "net.gradient_descent(x_train, y_label, epochs=500, learning_rate=0.01)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(y_label[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_label[0:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79c9b4dc955595e7f11f70f4f1c2000e30fde47a19708ab60dcdade70e6665e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
